name: Test Suite

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/**'
      - '.github/workflows/**'
      - 'requirements*.txt'
      - 'package*.json'
      - 'Dockerfile*'
      - 'docker-compose*.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/**'
      - '.github/workflows/**'
      - 'requirements*.txt'
      - 'package*.json'
      - 'Dockerfile*'
      - 'docker-compose*.yml'
  schedule:
    # Run full test suite daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - backend
        - frontend
        - ml
        - performance
        - security
        - smoke
      run_load_tests:
        description: 'Run load tests'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  POETRY_VERSION: '1.6.1'
  
  # Test configuration
  PYTEST_WORKERS: 'auto'
  COVERAGE_THRESHOLD: 85
  
  # Database configuration for tests
  TEST_DATABASE_URL: postgresql://test_user:test_pass@localhost:5432/test_ipo_valuation
  TEST_REDIS_URL: redis://localhost:6379/1
  
  # Security
  TEST_JWT_SECRET: test-jwt-secret-for-ci-only
  TEST_ENCRYPTION_KEY: test-encryption-key-32-characters
  
  # GCP Test Configuration
  GOOGLE_CLOUD_PROJECT: ipo-valuation-test
  GCP_CREDENTIALS_JSON: ${{ secrets.GCP_TEST_CREDENTIALS }}
  
  # External API test configuration
  TEST_EXTERNAL_SERVICES: false
  API_RATE_LIMIT_ENABLED: false

jobs:
  # Job to determine what tests to run based on changes
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.changes.outputs.backend }}
      frontend: ${{ steps.changes.outputs.frontend }}
      ml: ${{ steps.changes.outputs.ml }}
      security: ${{ steps.changes.outputs.security }}
      performance: ${{ steps.changes.outputs.performance }}
      docs: ${{ steps.changes.outputs.docs }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - uses: dorny/paths-filter@v2
        id: changes
        with:
          filters: |
            backend:
              - 'src/backend/**'
              - 'tests/backend/**'
              - 'requirements*.txt'
              - 'alembic/**'
            frontend:
              - 'src/frontend/**'
              - 'tests/frontend/**'
              - 'package*.json'
              - '*.js'
              - '*.ts'
            ml:
              - 'src/backend/ml_services/**'
              - 'src/backend/nlp_services/**'
              - 'tests/backend/ml/**'
              - 'models/**'
            security:
              - 'security/**'
              - 'tests/security/**'
              - 'src/backend/api/middleware/**'
            performance:
              - 'tests/performance/**'
              - 'docker-compose*.yml'
              - 'src/backend/api/**'
            docs:
              - 'docs/**'
              - '*.md'
              - 'tests/README.md'

  # Backend testing job
  test-backend:
    needs: detect-changes
    if: needs.detect-changes.outputs.backend == 'true' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'backend'
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_pass
          POSTGRES_USER: test_user
          POSTGRES_DB: test_ipo_valuation
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements-test.txt
          pip install -r src/backend/requirements/requirements.txt

      - name: Set up test database
        run: |
          # Wait for PostgreSQL to be ready
          until pg_isready -h localhost -p 5432 -U test_user; do
            sleep 1
          done
          
          # Run migrations
          cd src/backend
          alembic upgrade head

      - name: Lint backend code
        run: |
          flake8 src/backend --count --select=E9,F63,F7,F82 --show-source --statistics
          black --check src/backend
          isort --check-only src/backend

      - name: Type checking
        run: |
          mypy src/backend --ignore-missing-imports

      - name: Run backend unit tests
        run: |
          cd tests
          pytest backend/unit/ \
            --cov=../src/backend \
            --cov-report=xml:reports/backend-unit-coverage.xml \
            --cov-report=html:coverage/backend-unit \
            --junit-xml=reports/backend-unit-results.xml \
            -v --tb=short

      - name: Run backend integration tests
        run: |
          cd tests
          pytest backend/integration/ \
            --cov=../src/backend \
            --cov-report=xml:reports/backend-integration-coverage.xml \
            --junit-xml=reports/backend-integration-results.xml \
            -v

      - name: Run API tests
        run: |
          cd tests
          pytest backend/api/ \
            --cov=../src/backend/api \
            --junit-xml=reports/backend-api-results.xml \
            -v

      - name: Upload backend test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: backend-test-results
          path: |
            tests/reports/backend-*
            tests/coverage/backend-*

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: tests/reports/backend-unit-coverage.xml
          flags: backend
          name: backend-coverage

  # Frontend testing job
  test-frontend:
    needs: detect-changes
    if: needs.detect-changes.outputs.frontend == 'true' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'frontend'
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: tests/frontend/package-lock.json

      - name: Install dependencies
        run: |
          cd tests/frontend
          npm ci

      - name: Install Playwright browsers
        run: |
          cd tests/frontend
          npx playwright install --with-deps

      - name: Lint frontend code
        run: |
          cd tests/frontend
          npm run lint

      - name: Type checking
        run: |
          cd tests/frontend
          npm run typecheck

      - name: Run frontend unit tests
        run: |
          cd tests/frontend
          npm run test:coverage

      - name: Run E2E tests
        run: |
          cd tests/frontend
          npm run test:e2e

      - name: Run accessibility tests
        run: |
          cd tests/frontend
          npm run test:accessibility

      - name: Upload frontend test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: frontend-test-results
          path: |
            tests/frontend/coverage/
            tests/frontend/test-results/
            tests/frontend/playwright-report/

      - name: Upload Playwright report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-report
          path: tests/frontend/playwright-report/

  # ML/AI testing job
  test-ml:
    needs: detect-changes
    if: needs.detect-changes.outputs.ml == 'true' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'ml'
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-ml-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-ml-
            ${{ runner.os }}-pip-

      - name: Install ML dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements-test.txt
          pip install -r src/backend/requirements/requirements.txt
          # Additional ML libraries for testing
          pip install great-expectations evidently deepchecks

      - name: Download test models (if available)
        run: |
          # Download pre-trained models for testing
          mkdir -p models/test/
          # This would download actual models in a real scenario
          echo "Mock model files for testing" > models/test/README.txt

      - name: Run ML model tests
        run: |
          cd tests
          pytest backend/ml/ \
            --junit-xml=reports/ml-results.xml \
            -v --tb=short \
            -m "not slow"

      - name: Run ML accuracy tests
        run: |
          cd tests
          pytest backend/ml/test_model_accuracy.py \
            --junit-xml=reports/ml-accuracy-results.xml \
            -v

      - name: Run data drift tests
        run: |
          cd tests
          pytest backend/ml/ \
            -k "drift" \
            --junit-xml=reports/ml-drift-results.xml \
            -v

      - name: Upload ML test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: ml-test-results
          path: tests/reports/ml-*

  # Performance testing job
  test-performance:
    needs: detect-changes
    if: needs.detect-changes.outputs.performance == 'true' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'performance' || github.event.inputs.run_load_tests == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_pass
          POSTGRES_USER: test_user
          POSTGRES_DB: test_ipo_valuation
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements-test.txt
          pip install locust

      - name: Start application for testing
        run: |
          cd src/backend
          # Start the application in the background
          python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 &
          sleep 10

      - name: Run load tests (CI mode)
        run: |
          cd tests/performance
          locust -f load_testing.py --headless \
            --users 20 --spawn-rate 5 -t 2m \
            --host http://localhost:8000 \
            --html ../../reports/load-test-report.html \
            --csv ../../reports/load-test

      - name: Run performance benchmarks
        run: |
          cd tests
          pytest performance/ \
            --benchmark-only \
            --benchmark-json=reports/benchmarks.json \
            --junit-xml=reports/performance-results.xml

      - name: Upload performance test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: |
            tests/reports/load-test-*
            tests/reports/benchmarks.json
            tests/reports/performance-results.xml

  # Security testing job
  test-security:
    needs: detect-changes
    if: needs.detect-changes.outputs.security == 'true' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'security'
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements-test.txt
          pip install bandit safety semgrep

      - name: Run security vulnerability tests
        run: |
          cd tests
          pytest security/ \
            --junit-xml=reports/security-results.xml \
            -v

      - name: Run Bandit security scan
        run: |
          bandit -r src/backend -f json -o tests/reports/security-scan.json || true
          bandit -r src/backend -f html -o tests/reports/security-scan.html || true

      - name: Run Safety check
        run: |
          safety check --json --output tests/reports/safety-scan.json || true

      - name: Run Semgrep security scan
        run: |
          semgrep --config=auto --json --output=tests/reports/semgrep-scan.json src/backend || true

      - name: Upload security test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-test-results
          path: |
            tests/reports/security-*
            tests/reports/safety-scan.json
            tests/reports/semgrep-scan.json

  # Integration testing job
  test-integration:
    needs: [test-backend, test-frontend]
    if: always() && !cancelled()
    runs-on: ubuntu-latest
    timeout-minutes: 20

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_pass
          POSTGRES_USER: test_user
          POSTGRES_DB: test_ipo_valuation
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements-test.txt
          cd tests/frontend
          npm ci
          npx playwright install --with-deps

      - name: Run integration tests
        run: |
          cd tests
          pytest integration/ \
            --junit-xml=reports/integration-results.xml \
            -v -m "not external"

      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: tests/reports/integration-results.xml

  # Smoke tests for quick validation
  smoke-tests:
    if: github.event.inputs.test_type == 'smoke'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_pass
          POSTGRES_USER: test_user
          POSTGRES_DB: test_ipo_valuation
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements-test.txt

      - name: Run smoke tests
        run: |
          cd tests
          pytest \
            -x --maxfail=5 \
            -m "not slow and not external" \
            --junit-xml=reports/smoke-test-results.xml

      - name: Upload smoke test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: smoke-test-results
          path: tests/reports/smoke-test-results.xml

  # Quality gates job
  quality-gates:
    needs: [test-backend, test-frontend, test-ml, test-security]
    if: always() && !cancelled()
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install report tools
        run: |
          pip install junitparser coverage

      - name: Combine test results
        run: |
          # Create reports directory
          mkdir -p combined-reports
          
          # Combine JUnit XML files
          find . -name "*-results.xml" -exec cp {} combined-reports/ \;
          
          # Generate combined HTML report
          echo "<h1>Test Results Summary</h1>" > combined-reports/test-summary.html
          
          # Count passed/failed tests
          python -c "
          import glob
          from junitparser import JUnitXml
          
          total_tests = 0
          total_failures = 0
          total_errors = 0
          
          for xml_file in glob.glob('combined-reports/*.xml'):
              try:
                  xml = JUnitXml.fromfile(xml_file)
                  total_tests += xml.tests
                  total_failures += xml.failures
                  total_errors += xml.errors
              except:
                  pass
          
          print(f'Total Tests: {total_tests}')
          print(f'Failures: {total_failures}')
          print(f'Errors: {total_errors}')
          print(f'Success Rate: {((total_tests - total_failures - total_errors) / total_tests * 100):.1f}%' if total_tests > 0 else 'N/A')
          
          # Write to summary file
          with open('combined-reports/summary.txt', 'w') as f:
              f.write(f'Total Tests: {total_tests}\n')
              f.write(f'Failures: {total_failures}\n')
              f.write(f'Errors: {total_errors}\n')
              f.write(f'Success Rate: {((total_tests - total_failures - total_errors) / total_tests * 100):.1f}%\n' if total_tests > 0 else 'Success Rate: N/A\n')
          "

      - name: Check quality gates
        run: |
          # Read test summary
          if [ -f combined-reports/summary.txt ]; then
            cat combined-reports/summary.txt
            
            # Extract success rate
            success_rate=$(grep "Success Rate:" combined-reports/summary.txt | cut -d' ' -f3 | tr -d '%')
            
            if [ ! -z "$success_rate" ] && [ "$success_rate" != "N/A" ]; then
              if (( $(echo "$success_rate < 95" | bc -l) )); then
                echo "Quality Gate Failed: Success rate $success_rate% is below 95%"
                exit 1
              else
                echo "Quality Gate Passed: Success rate $success_rate%"
              fi
            fi
          fi

      - name: Upload combined test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: combined-test-results
          path: combined-reports/

      - name: Comment PR with test results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            if (fs.existsSync('combined-reports/summary.txt')) {
              const summary = fs.readFileSync('combined-reports/summary.txt', 'utf8');
              
              const body = `## Test Results Summary
              
              \`\`\`
              ${summary}
              \`\`\`
              
              - ✅ Backend Tests
              - ✅ Frontend Tests  
              - ✅ ML/AI Tests
              - ✅ Security Tests
              - ✅ Integration Tests
              
              View detailed results in the [Actions tab](${context.payload.pull_request.html_url.replace('/pull/', '/actions')}).`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            }

  # Nightly full test run
  nightly-tests:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 90

    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
        
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Run full test suite
        run: |
          make install-deps
          make setup-test-env
          make test-all

      - name: Generate nightly report
        run: |
          echo "Nightly test run completed for Python ${{ matrix.python-version }}" > nightly-report.txt
          date >> nightly-report.txt

      - name: Upload nightly results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: nightly-results-py${{ matrix.python-version }}
          path: |
            tests/reports/
            nightly-report.txt